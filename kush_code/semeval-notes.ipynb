{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. conda install all strox dependencies\n",
    "2. use jupyter not colab \n",
    "3. add conda env package list to commit\n",
    "4. ok imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = '/opt/anaconda3/stanford-postagger-full-2020-08-06'\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk import word_tokenize\n",
    "_path_to_model = home + '/models/english-bidirectional-distsim.tagger' \n",
    "_path_to_jar = home + '/stanford-postagger.jar'\n",
    "stanford_tag = POS_Tag(model_filename=_path_to_model, path_to_jar=_path_to_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'ABSA16_Laptops_Train_English_SB2.xml'\n",
    "path_test = 'EN_LAPT_SB2_TEST.xml.gold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so [joblib](https://joblib.readthedocs.io/en/latest/index.html) is python pipeline processing lib\n",
    "\n",
    "sklearn is used here. this is a maintained and reliable library according to [this guy](https://www.youtube.com/watch?v=rvVkVsG49uU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get \n",
    "    - ABSA16_Laptops_Train_English_SB2.xml \n",
    "    - EN_LAPT_SB2_TEST.xml\n",
    "    - stanford pos tagger stuff\n",
    "    - tagged_text_list_train.pkl files\n",
    "- add function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(path):\n",
    "    tree=ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    text_list = []\n",
    "    opinion_list = []\n",
    "    for review in root.findall('Review'):\n",
    "        text_string=\"\"\n",
    "        opinion_inner_list=[]\n",
    "        for sent in review.findall('./sentences/sentence'):\n",
    "            text_string= text_string+ \" \"+ sent.find('text').text\n",
    "        text_list.append(text_string)\n",
    "        for opinion in review.findall('./Opinions/Opinion'):\n",
    "            opinion_dict = {\n",
    "                opinion.get('category').replace('#','_'): opinion.get('polarity')\n",
    "            }\n",
    "            opinion_inner_list.append(opinion_dict)\n",
    "        opinion_list.append(opinion_inner_list)\n",
    "    return text_list,opinion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_aspect(opinion_list):\n",
    "    import nltk\n",
    "    opinion= []\n",
    "    for inner_list in opinion_list:\n",
    "        for _dict in inner_list:\n",
    "            for key in _dict:\n",
    "                opinion.append(key)\n",
    "    most_common_aspect = [k for k,v in nltk.FreqDist(opinion).most_common(20)]\n",
    "    return most_common_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_frame(text_list,opinion_list,most_common_aspect):\n",
    "    data={'Review':text_list}\n",
    "    df = pd.DataFrame(data)\n",
    "    if opinion_list:\n",
    "        for inner_list in opinion_list:\n",
    "            for _dict in inner_list:\n",
    "                for key in _dict:\n",
    "                    if key in most_common_aspect:\n",
    "                        df.loc[opinion_list.index(inner_list),key]=_dict[key]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive','negative','neutral','conflict'],[1,1,1,1])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_data_frame_test(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        if df.get(common_aspect,0):\n",
    "            df[common_aspect]=df[common_aspect].replace(['positive','negative','neutral','conflict'],[1,1,1,1])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive'],[1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative','neutral','conflict'],[0,0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative'],[1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive','neutral','conflict'],[0,0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neutral_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['neutral','conflict'],[1,1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative','positive'],[0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTag(review):\n",
    "    tagged_text_list=[]\n",
    "    for text in review:\n",
    "        tagged_text_list.append(stanford_tag.tag(word_tokenize(text)))\n",
    "    return tagged_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_aspect(y,most_common_aspect):\n",
    "    position=[]\n",
    "    for innerlist in y:\n",
    "        position.append([i for i, j in enumerate(innerlist) if j == 1])\n",
    "    sorted_common=sorted(most_common_aspect)\n",
    "    dict_aspect=[]\n",
    "    for innerlist in position:\n",
    "        inner_dict={}\n",
    "        for word in sorted_common:\n",
    "            if sorted_common.index(word) in innerlist:\n",
    "                inner_dict[word]= 5\n",
    "            else:\n",
    "                inner_dict[word]=0\n",
    "        dict_aspect.append(inner_dict)\n",
    "    return dict_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(l1,l2=None,n=1):\n",
    "    for i in range(0,n):\n",
    "        rand_int = int(np.random.rand() * len(l1))\n",
    "        if l2:\n",
    "            print(l1[rand_int]), '\\n\\n', print(l2[rand_int])\n",
    "        else:\n",
    "            print(l1[rand_int])\n",
    "            return l1[rand_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(l):\n",
    "    print('type: ',type(l))\n",
    "    print('len: ',len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[thestrox Github code](https://github.com/thestrox/Aspect-Based-Sentiment-Analysis/blob/master/Aspect%20Based%20Sentiment%20Analysis.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text_list, train_opinion_list = get_list(path_train)\n",
    "# get_sample(train_text_list,train_opinion_list,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_aspect = get_most_common_aspect(train_opinion_list)\n",
    "# most_common_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text_list_train=joblib.load('tagged_text_list_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_text_list = filterTag(tagged_text_list_train)\n",
    "# get_sample(tagged_text_list_train,final_train_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each train review and its SB2 (aspect,sentiment) pair is POS-tagged, and only nouns, adjectives, verbs and adverbs are retained according to [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) POS table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_data_frame(train_text_list,train_opinion_list,most_common_aspect)\n",
    "# df_train.sample(3)\n",
    "# df_train.describe()\n",
    "# df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_aspect = get_aspect_data_frame(df_train,most_common_aspect)\n",
    "# df_train_aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list,test_opinion_list = get_list(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text_list_test=joblib.load('tagged_text_list_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_text_list=filterTag(tagged_text_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_data_frame(final_test_text_list,test_opinion_list,most_common_aspect)\n",
    "df_test_aspect = get_aspect_data_frame(df_test,most_common_aspect)\n",
    "df_test_aspect = df_test_aspect.reindex(sorted(df_test_aspect.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= df_train_aspect.Review\n",
    "y_train = df_train_aspect.drop('Review',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test_aspect.Review\n",
    "y_test = df_test_aspect.drop('Review',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train = np.asarray(y_train, dtype=np.int64)\n",
    "y_test = np.asarray(y_test, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "vect = CountVectorizer(max_df=1.0,stop_words='english')  \n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classif = OneVsRestClassifier(MultinomialNB()).fit(X_train_dtm, y_train)\n",
    "C = 1.0 #SVregularization parameter\n",
    "svc = OneVsRestClassifier(svm.SVC(kernel='linear', C=C)).fit(X_train_dtm, y_train)\n",
    "lin_svc = OneVsRestClassifier(svm.LinearSVC(C=C)).fit(X_train_dtm, y_train)\n",
    "sgd = OneVsRestClassifier(SGDClassifier()).fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = nb_classif.predict(X_test_dtm)\n",
    "y_pred_class_svc = svc.predict(X_test_dtm)\n",
    "y_pred_class_lin_svc = lin_svc.predict(X_test_dtm)\n",
    "y_pred_class_sgd = sgd.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,y_pred_class))\n",
    "print(metrics.accuracy_score(y_test,y_pred_class_svc))\n",
    "print(metrics.accuracy_score(y_test,y_pred_class_lin_svc))\n",
    "print(metrics.accuracy_score(y_test,y_pred_class_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22672064777327935\n",
      "0.24598930481283424\n",
      "0.2349570200573066\n",
      "0.22968197879858657\n"
     ]
    }
   ],
   "source": [
    "print(metrics.precision_score(y_test,y_pred_class,average='micro'))\n",
    "print(metrics.precision_score(y_test,y_pred_class_svc,average='micro'))\n",
    "print(metrics.precision_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "print(metrics.precision_score(y_test,y_pred_class_sgd,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13559322033898305\n",
      "0.22276029055690072\n",
      "0.19854721549636803\n",
      "0.15738498789346247\n"
     ]
    }
   ],
   "source": [
    "print(metrics.recall_score(y_test,y_pred_class,average='micro'))\n",
    "print(metrics.recall_score(y_test,y_pred_class_svc,average='micro'))\n",
    "print(metrics.recall_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "print(metrics.recall_score(y_test,y_pred_class_sgd,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1696969696969697\n",
      "0.23379923761118174\n",
      "0.2152230971128609\n",
      "0.18678160919540232\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(y_test,y_pred_class,average='micro'))\n",
    "print(metrics.f1_score(y_test,y_pred_class_svc,average='micro'))\n",
    "print(metrics.f1_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "print(metrics.f1_score(y_test,y_pred_class_sgd,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      1.00      0.30        14\n",
      "           1       0.22      0.46      0.30        24\n",
      "           2       0.11      0.17      0.13        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.45      0.71      0.56        21\n",
      "           5       0.06      0.12      0.08         8\n",
      "           6       0.20      0.43      0.27         7\n",
      "           7       0.14      0.05      0.08        39\n",
      "           8       1.00      0.06      0.12        80\n",
      "           9       1.00      0.04      0.08        24\n",
      "          10       1.00      0.02      0.04        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        27\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.12      0.25      0.17         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.23      0.14      0.17       413\n",
      "   macro avg       0.22      0.17      0.11       413\n",
      "weighted avg       0.43      0.14      0.11       413\n",
      " samples avg       0.20      0.13      0.15       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      1.00      0.31        14\n",
      "           1       0.28      0.58      0.38        24\n",
      "           2       0.17      0.50      0.26        12\n",
      "           3       0.11      0.50      0.18         4\n",
      "           4       0.28      0.48      0.35        21\n",
      "           5       0.15      0.38      0.21         8\n",
      "           6       0.08      0.29      0.13         7\n",
      "           7       0.36      0.23      0.28        39\n",
      "           8       1.00      0.06      0.12        80\n",
      "           9       0.25      0.04      0.07        24\n",
      "          10       0.60      0.07      0.12        46\n",
      "          11       0.17      0.60      0.26         5\n",
      "          12       0.50      0.07      0.13        27\n",
      "          13       1.00      0.14      0.24        29\n",
      "          14       0.50      0.13      0.21        30\n",
      "          15       0.15      0.75      0.25         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.67      0.40      0.50        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.17      0.09      0.12        11\n",
      "\n",
      "   micro avg       0.25      0.22      0.23       413\n",
      "   macro avg       0.33      0.32      0.21       413\n",
      "weighted avg       0.53      0.22      0.20       413\n",
      " samples avg       0.19      0.19      0.18       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      1.00      0.31        14\n",
      "           1       0.29      0.54      0.38        24\n",
      "           2       0.15      0.42      0.22        12\n",
      "           3       0.07      0.25      0.11         4\n",
      "           4       0.28      0.48      0.35        21\n",
      "           5       0.17      0.38      0.23         8\n",
      "           6       0.08      0.29      0.13         7\n",
      "           7       0.33      0.23      0.27        39\n",
      "           8       1.00      0.06      0.12        80\n",
      "           9       0.50      0.04      0.08        24\n",
      "          10       0.50      0.04      0.08        46\n",
      "          11       0.17      0.60      0.26         5\n",
      "          12       1.00      0.04      0.07        27\n",
      "          13       1.00      0.07      0.13        29\n",
      "          14       0.40      0.07      0.11        30\n",
      "          15       0.16      0.75      0.26         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.67      0.40      0.50        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.23      0.20      0.22       413\n",
      "   macro avg       0.35      0.28      0.18       413\n",
      "weighted avg       0.55      0.20      0.17       413\n",
      " samples avg       0.19      0.17      0.17       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      1.00      0.31        14\n",
      "           1       0.30      0.54      0.39        24\n",
      "           2       0.15      0.33      0.21        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.40      0.48      0.43        21\n",
      "           5       0.13      0.25      0.17         8\n",
      "           6       0.10      0.29      0.15         7\n",
      "           7       0.35      0.21      0.26        39\n",
      "           8       1.00      0.04      0.07        80\n",
      "           9       0.25      0.04      0.07        24\n",
      "          10       0.67      0.04      0.08        46\n",
      "          11       0.18      0.40      0.25         5\n",
      "          12       0.00      0.00      0.00        27\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       0.25      0.03      0.06        30\n",
      "          15       0.12      0.25      0.17         4\n",
      "          16       0.50      0.11      0.18         9\n",
      "          17       0.33      0.07      0.11        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.23      0.16      0.19       413\n",
      "   macro avg       0.25      0.20      0.15       413\n",
      "weighted avg       0.41      0.16      0.14       413\n",
      " samples avg       0.22      0.15      0.16       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_svc))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_lin_svc))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict_aspect=get_dict_aspect(y_train, most_common_aspect)\n",
    "d_train=DictVectorizer() \n",
    "X_train_aspect_dtm = d_train.fit_transform(train_dict_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_aspect=get_dict_aspect(y_test,most_common_aspect)\n",
    "d_test=DictVectorizer() \n",
    "X_test_aspect_dtm = d_test.fit_transform(test_dict_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(df_train,df_test,X_train_aspect_dtm,X_test_aspect_dtm):\n",
    "    \n",
    "    df_train = df_train.reindex(sorted(df_train_positive.columns), axis=1)\n",
    "    df_test = df_test.reindex(sorted(df_test_positive.columns), axis=1)\n",
    "\n",
    "    import numpy as np\n",
    "    X_train = df_train.Review\n",
    "    y_train = df_train.drop('Review',1)\n",
    "    y_train = np.asarray(y_train, dtype=np.int64)\n",
    "\n",
    "    X_test = df_test.Review\n",
    "    y_test = df_test.drop('Review',1)\n",
    "    y_test = np.asarray(y_test, dtype=np.int64)\n",
    "\n",
    "    vect_sen = CountVectorizer(stop_words='english',ngram_range=(1,2))  \n",
    "    X_train_dtm = vect_sen.fit_transform(X_train)\n",
    "    X_test_dtm = vect_sen.transform(X_test)\n",
    "\n",
    "    #ombining word vector with extra feature.\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_dtm=hstack((X_train_dtm, X_train_aspect_dtm))\n",
    "    X_test_dtm=hstack((X_test_dtm, X_test_aspect_dtm))\n",
    "\n",
    "    C = 1.0 #SVregularization parameter\n",
    "    nb_classif = OneVsRestClassifier(MultinomialNB()).fit(X_train_dtm, y_train)\n",
    "    svc = OneVsRestClassifier(svm.SVC(kernel='linear', C=C)).fit(X_train_dtm, y_train)\n",
    "    lin_svc = OneVsRestClassifier(svm.LinearSVC(C=C)).fit(X_train_dtm, y_train)\n",
    "    sgd = OneVsRestClassifier(SGDClassifier()).fit(X_train_dtm,y_train)\n",
    "\n",
    "    y_pred_class= nb_classif.predict(X_test_dtm)\n",
    "    y_pred_class_svc = svc.predict(X_test_dtm)\n",
    "    y_pred_class_lin_svc = lin_svc.predict(X_test_dtm)\n",
    "    y_pred_class_sgd = sgd.predict(X_test_dtm)\n",
    "    return (y_test,y_pred_class,y_pred_class_svc,y_pred_class_lin_svc,y_pred_class_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrices(y_test,y_pred_class,y_pred_class_svc,y_pred_class_lin_svc,y_pred_class_sgd):\n",
    "    print(\"Accuracy:\")\n",
    "    print(metrics.accuracy_score(y_test,y_pred_class))\n",
    "    print(metrics.accuracy_score(y_test,y_pred_class_svc))\n",
    "    print(metrics.accuracy_score(y_test,y_pred_class_lin_svc))\n",
    "    print(metrics.accuracy_score(y_test,y_pred_class_sgd))\n",
    "\n",
    "    print(\"\\nAverage precision:\")\n",
    "    print(metrics.precision_score(y_test,y_pred_class,average='micro'))\n",
    "    print(metrics.precision_score(y_test,y_pred_class_svc,average='micro'))\n",
    "    print(metrics.precision_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "    print(metrics.precision_score(y_test,y_pred_class_sgd,average='micro'))\n",
    "\n",
    "    print(\"\\nAverage recall:\")\n",
    "    print(metrics.recall_score(y_test,y_pred_class,average='micro'))\n",
    "    print(metrics.recall_score(y_test,y_pred_class_svc,average='micro'))\n",
    "    print(metrics.recall_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "    print(metrics.recall_score(y_test,y_pred_class_sgd,average='micro'))\n",
    "    \n",
    "    print(\"\\nAverage f1:\")\n",
    "    print(metrics.f1_score(y_test,y_pred_class,average='micro'))\n",
    "    print(metrics.f1_score(y_test,y_pred_class_svc,average='micro'))\n",
    "    print(metrics.f1_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "    print(metrics.f1_score(y_test,y_pred_class_sgd,average='micro'))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_svc))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_lin_svc))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Average precision:\n",
      "0.821917808219178\n",
      "0.23737373737373738\n",
      "0.21764705882352942\n",
      "0.24394463667820068\n",
      "\n",
      "Average recall:\n",
      "0.21739130434782608\n",
      "0.17028985507246377\n",
      "0.26811594202898553\n",
      "0.5108695652173914\n",
      "\n",
      "Average f1:\n",
      "0.3438395415472779\n",
      "0.19831223628691982\n",
      "0.24025974025974028\n",
      "0.33021077283372363\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00        15\n",
      "           5       0.00      0.00      0.00         7\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.00      0.00      0.00        26\n",
      "           8       0.82      0.95      0.88        61\n",
      "           9       0.00      0.00      0.00        11\n",
      "          10       1.00      0.06      0.11        36\n",
      "          11       0.00      0.00      0.00         4\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.00      0.00      0.00        18\n",
      "          14       0.00      0.00      0.00        23\n",
      "          15       0.00      0.00      0.00         3\n",
      "          16       0.00      0.00      0.00         7\n",
      "          17       0.00      0.00      0.00         8\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.82      0.22      0.34       276\n",
      "   macro avg       0.09      0.05      0.05       276\n",
      "weighted avg       0.31      0.22      0.21       276\n",
      " samples avg       0.72      0.21      0.31       276\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.17      0.25        12\n",
      "           1       0.44      0.29      0.35        14\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.03      0.33      0.06         3\n",
      "           4       0.00      0.00      0.00        15\n",
      "           5       0.21      0.71      0.32         7\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.29      0.08      0.12        26\n",
      "           8       1.00      0.25      0.39        61\n",
      "           9       0.00      0.00      0.00        11\n",
      "          10       0.42      0.22      0.29        36\n",
      "          11       0.04      0.25      0.06         4\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.75      0.17      0.27        18\n",
      "          14       0.67      0.09      0.15        23\n",
      "          15       0.00      0.00      0.00         3\n",
      "          16       0.17      0.14      0.15         7\n",
      "          17       0.25      0.25      0.25         8\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.06      0.25      0.10         4\n",
      "\n",
      "   micro avg       0.24      0.17      0.20       276\n",
      "   macro avg       0.24      0.16      0.14       276\n",
      "weighted avg       0.47      0.17      0.22       276\n",
      " samples avg       0.17      0.12      0.13       276\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.17      0.21        12\n",
      "           1       0.21      0.43      0.29        14\n",
      "           2       0.00      0.00      0.00         6\n",
      "           3       0.04      0.33      0.06         3\n",
      "           4       0.00      0.00      0.00        15\n",
      "           5       0.15      0.57      0.24         7\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.35      0.23      0.28        26\n",
      "           8       0.96      0.39      0.56        61\n",
      "           9       0.33      0.09      0.14        11\n",
      "          10       0.55      0.44      0.49        36\n",
      "          11       0.03      0.25      0.05         4\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.17      0.11      0.13        18\n",
      "          14       0.67      0.17      0.28        23\n",
      "          15       0.00      0.00      0.00         3\n",
      "          16       0.31      0.57      0.40         7\n",
      "          17       0.11      0.12      0.12         8\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.04      0.50      0.07         4\n",
      "\n",
      "   micro avg       0.22      0.27      0.24       276\n",
      "   macro avg       0.21      0.22      0.17       276\n",
      "weighted avg       0.44      0.27      0.30       276\n",
      " samples avg       0.21      0.20      0.19       276\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.08      0.07        12\n",
      "           1       0.21      0.57      0.31        14\n",
      "           2       0.33      0.50      0.40         6\n",
      "           3       0.03      0.33      0.05         3\n",
      "           4       0.20      0.60      0.30        15\n",
      "           5       0.09      0.71      0.15         7\n",
      "           6       0.04      0.25      0.06         4\n",
      "           7       0.27      0.15      0.20        26\n",
      "           8       0.78      0.84      0.81        61\n",
      "           9       0.33      0.09      0.14        11\n",
      "          10       0.52      0.81      0.63        36\n",
      "          11       0.06      0.75      0.12         4\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.27      0.22      0.24        18\n",
      "          14       0.42      0.48      0.45        23\n",
      "          15       0.16      1.00      0.27         3\n",
      "          16       0.67      0.29      0.40         7\n",
      "          17       0.09      0.38      0.14         8\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.05      0.50      0.10         4\n",
      "\n",
      "   micro avg       0.24      0.51      0.33       276\n",
      "   macro avg       0.23      0.43      0.24       276\n",
      "weighted avg       0.39      0.51      0.41       276\n",
      " samples avg       0.26      0.44      0.28       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "df_test = get_data_frame(final_test_text_list,test_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_positive = get_positive_data_frame(df_train,most_common_aspect)\n",
    "df_test_positive = get_positive_data_frame(df_test,most_common_aspect)\n",
    "y_test_pos,y_pred_class_pos,y_pred_class_svc_pos,y_pred_class_lin_svc_pos,y_pred_class_sgd_pos=classify_sentiment(df_train_positive,df_test_positive,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print_metrices(y_test_pos,y_pred_class_pos,y_pred_class_svc_pos,y_pred_class_lin_svc_pos,y_pred_class_sgd_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.5\n",
      "0.25\n",
      "0.0625\n",
      "0.0375\n",
      "\n",
      "Average precision:\n",
      "0.8\n",
      "0.09090909090909091\n",
      "0.10227272727272728\n",
      "0.0764872521246459\n",
      "\n",
      "Average recall:\n",
      "0.03669724770642202\n",
      "0.07339449541284404\n",
      "0.24770642201834864\n",
      "0.24770642201834864\n",
      "\n",
      "Average f1:\n",
      "0.07017543859649122\n",
      "0.08121827411167513\n",
      "0.14477211796246647\n",
      "0.11688311688311688\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.00      0.00      0.00        10\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         3\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.80      0.22      0.35        18\n",
      "           9       0.00      0.00      0.00         9\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.00      0.00      0.00        11\n",
      "          14       0.00      0.00      0.00         6\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.00      0.00      0.00         5\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.00      0.00      0.00         7\n",
      "\n",
      "   micro avg       0.80      0.04      0.07       109\n",
      "   macro avg       0.04      0.01      0.02       109\n",
      "weighted avg       0.13      0.04      0.06       109\n",
      " samples avg       0.05      0.01      0.02       109\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.06      0.10      0.07        10\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         3\n",
      "           7       0.17      0.33      0.22         3\n",
      "           8       0.20      0.06      0.09        18\n",
      "           9       0.00      0.00      0.00         9\n",
      "          10       0.33      0.10      0.15        10\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       1.00      0.08      0.15        12\n",
      "          13       0.00      0.00      0.00        11\n",
      "          14       0.00      0.00      0.00         6\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.00      0.00      0.00         5\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.10      0.43      0.16         7\n",
      "\n",
      "   micro avg       0.09      0.07      0.08       109\n",
      "   macro avg       0.09      0.06      0.04       109\n",
      "weighted avg       0.19      0.07      0.07       109\n",
      " samples avg       0.06      0.05      0.04       109\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.12      0.30      0.17        10\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.09      1.00      0.17         1\n",
      "           6       0.00      0.00      0.00         3\n",
      "           7       0.06      0.33      0.10         3\n",
      "           8       0.31      0.22      0.26        18\n",
      "           9       0.11      0.11      0.11         9\n",
      "          10       0.12      0.20      0.15        10\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.27      0.25      0.26        12\n",
      "          13       0.30      0.27      0.29        11\n",
      "          14       0.22      0.33      0.27         6\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.11      0.50      0.18         2\n",
      "          17       0.00      0.00      0.00         5\n",
      "          18       0.06      0.50      0.11         2\n",
      "          19       0.08      0.71      0.14         7\n",
      "\n",
      "   micro avg       0.10      0.25      0.14       109\n",
      "   macro avg       0.09      0.24      0.11       109\n",
      "weighted avg       0.17      0.25      0.17       109\n",
      " samples avg       0.08      0.12      0.09       109\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.07      0.20      0.11        10\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.07      1.00      0.12         1\n",
      "           6       0.07      0.33      0.11         3\n",
      "           7       0.10      0.67      0.17         3\n",
      "           8       0.34      0.56      0.43        18\n",
      "           9       0.07      0.11      0.09         9\n",
      "          10       0.10      0.20      0.13        10\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00        12\n",
      "          13       0.00      0.00      0.00        11\n",
      "          14       0.00      0.00      0.00         6\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.09      0.50      0.15         2\n",
      "          17       0.00      0.00      0.00         5\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.11      1.00      0.20         7\n",
      "\n",
      "   micro avg       0.08      0.25      0.12       109\n",
      "   macro avg       0.05      0.23      0.08       109\n",
      "weighted avg       0.09      0.25      0.12       109\n",
      " samples avg       0.07      0.11      0.08       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "df_test = get_data_frame(final_test_text_list,test_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_neg = get_negative_data_frame(df_train,most_common_aspect)\n",
    "df_test_neg = get_negative_data_frame(df_test,most_common_aspect)\n",
    "\n",
    "y_test_neg,y_pred_class_neg,y_pred_class_svc_neg,y_pred_class_lin_svc_neg,y_pred_class_sgd_neg=classify_sentiment(df_train_neg,df_test_neg,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print_metrices(y_test_neg,y_pred_class_neg,y_pred_class_svc_neg,y_pred_class_lin_svc_neg,y_pred_class_sgd_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.7375\n",
      "0.6875\n",
      "0.275\n",
      "0.125\n",
      "\n",
      "Average precision:\n",
      "0.0\n",
      "0.1\n",
      "0.028985507246376812\n",
      "0.007575757575757576\n",
      "\n",
      "Average recall:\n",
      "0.0\n",
      "0.03571428571428571\n",
      "0.07142857142857142\n",
      "0.03571428571428571\n",
      "\n",
      "Average f1:\n",
      "0.0\n",
      "0.05263157894736841\n",
      "0.04123711340206185\n",
      "0.0125\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00        10\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         3\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        28\n",
      "   macro avg       0.00      0.00      0.00        28\n",
      "weighted avg       0.00      0.00      0.00        28\n",
      " samples avg       0.00      0.00      0.00        28\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00        10\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.11      0.33      0.17         3\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.10      0.04      0.05        28\n",
      "   macro avg       0.01      0.02      0.01        28\n",
      "weighted avg       0.01      0.04      0.02        28\n",
      " samples avg       0.01      0.01      0.01        28\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.05      0.10      0.06        10\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.03      0.33      0.05         3\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.03      0.07      0.04        28\n",
      "   macro avg       0.00      0.02      0.01        28\n",
      "weighted avg       0.02      0.07      0.03        28\n",
      " samples avg       0.02      0.02      0.02        28\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00        10\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.05      0.33      0.08         3\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.01      0.04      0.01        28\n",
      "   macro avg       0.00      0.02      0.00        28\n",
      "weighted avg       0.00      0.04      0.01        28\n",
      " samples avg       0.00      0.01      0.01        28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "df_test = get_data_frame(final_test_text_list,test_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_neu = get_neutral_data_frame(df_train,most_common_aspect)\n",
    "df_test_neu = get_neutral_data_frame(df_test,most_common_aspect)\n",
    "\n",
    "y_test_neu,y_pred_class_neu,y_pred_class_svc_neu,y_pred_class_lin_svc_neu,y_pred_class_sgd_neu=classify_sentiment(df_train_neu,df_test_neu,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print_metrices(y_test_neu,y_pred_class_neu,y_pred_class_svc_neu,y_pred_class_lin_svc_neu,y_pred_class_sgd_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input=\"This is my first asus laptop. So far i am really enjoying this laptop. 512GB SSD is super fast. Battery life is also good and can last very long. I have no complain on screen quality too as display supports 4k videos. Maybe that is why it costs a lot. This is an expensive laptop and it's price is very high compared to other laptops of similar specs. So, if you have no trouble paying for this laptop, it is pretty good.\"\n",
    "tagged_user_input = posTag([user_input])\n",
    "filter_tagged_user_input = filterTag(tagged_user_input)\n",
    "\n",
    "user_input_series=pd.Series(filter_tagged_user_input)\n",
    "user_input_series_dtm=vect.transform(user_input_series)\n",
    "\n",
    "predict_aspect= svc.predict(user_input_series_dtm)\n",
    "extra_feature=get_dict_aspect(predict_aspect, most_common_aspect)\n",
    "extra_feature_dtm=DictVectorizer().fit_transform(extra_feature)\n",
    "predict_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_opinion_list=[]\n",
    "df_test = get_data_frame(filter_tagged_user_input,test_opinion_list,most_common_aspect)\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_positive = get_positive_data_frame(df_train,most_common_aspect)\n",
    "y_test_pos,y_pred_class_pos,y_pred_class_svc_pos,y_pred_class_lin_svc_pos,y_pred_class_sgd_pos=classify_sentiment(df_train_positive,df_test,X_train_aspect_dtm,extra_feature_dtm)\n",
    "\n",
    "y_pred_class_svc_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_opinion_list=[]\n",
    "df_test = get_data_frame(filter_tagged_user_input,test_opinion_list,most_common_aspect)\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_negative = get_negative_data_frame(df_train,most_common_aspect)\n",
    "y_test_neg,y_pred_class_neg,y_pred_class_svc_neg,y_pred_class_lin_svc_neg,y_pred_class_sgd_neg=classify_sentiment(df_train_negative,df_test,X_train_aspect_dtm,extra_feature_dtm)\n",
    "\n",
    "y_pred_class_svc_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_opinion_list=[]\n",
    "df_test = get_data_frame(filter_tagged_user_input,test_opinion_list,most_common_aspect)\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_neutral = get_neutral_data_frame(df_train,most_common_aspect)\n",
    "y_test_neu,y_pred_class_neu,y_pred_class_svc_neu,y_pred_class_lin_svc_neu,y_pred_class_sgd_neu=classify_sentiment(df_train_neutral,df_test,X_train_aspect_dtm,extra_feature_dtm)\n",
    "\n",
    "y_pred_class_svc_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_positive=[]\n",
    "for i, (a, b) in enumerate(zip(predict_aspect.tolist()[0], y_pred_class_svc_pos.tolist()[0])):\n",
    "    if a ==1 and b==1:\n",
    "        index_positive.append(i)\n",
    "index_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_negative=[]\n",
    "for i, (a, b) in enumerate(zip(predict_aspect.tolist()[0], y_pred_class_svc_neg.tolist()[0])):\n",
    "    if a ==1 and b==1:\n",
    "        index_negative.append(i)\n",
    "index_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_neutral=[]\n",
    "for i, (a, b) in enumerate(zip(predict_aspect.tolist()[0], y_pred_class_svc_neu.tolist()[0])):\n",
    "    if a ==1 and b==1:\n",
    "        index_neutral.append(i)\n",
    "index_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_positive:\n",
    "    for index in index_positive:\n",
    "        output.append(sorted(most_common_aspect)[index]+\": positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_negative:\n",
    "    for index in index_negative:\n",
    "        output.append(sorted(most_common_aspect)[index]+\": negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_neutral:\n",
    "    for index in index_neutral:\n",
    "        output.append(sorted(most_common_aspect)[index]+\": neutral or conflict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BATTERY_OPERATION_PERFORMANCE: positive']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based aspects output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
