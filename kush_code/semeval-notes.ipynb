{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. conda install all strox dependencies\n",
    "2. use jupyter not colab \n",
    "3. add conda env package list to commit\n",
    "4. ok imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = '/opt/anaconda3/stanford-postagger-full-2020-08-06'\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk import word_tokenize\n",
    "_path_to_model = home + '/models/english-bidirectional-distsim.tagger' \n",
    "_path_to_jar = home + '/stanford-postagger.jar'\n",
    "stanford_tag = POS_Tag(model_filename=_path_to_model, path_to_jar=_path_to_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'ABSA16_Laptops_Train_English_SB2.xml'\n",
    "path_test = 'EN_LAPT_SB2_TEST.xml.A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so [joblib](https://joblib.readthedocs.io/en/latest/index.html) is python pipeline processing lib\n",
    "\n",
    "sklearn is used here. this is a maintained and reliable library according to [this guy](https://www.youtube.com/watch?v=rvVkVsG49uU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get \n",
    "    - ABSA16_Laptops_Train_English_SB2.xml \n",
    "    - EN_LAPT_SB2_TEST.xml\n",
    "    - stanford pos tagger stuff\n",
    "    - tagged_text_list_train.pkl files\n",
    "- add function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(path):\n",
    "    tree=ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    text_list = []\n",
    "    opinion_list = []\n",
    "    for review in root.findall('Review'):\n",
    "        text_string=\"\"\n",
    "        opinion_inner_list=[]\n",
    "        for sent in review.findall('./sentences/sentence'):\n",
    "            text_string= text_string+ \" \"+ sent.find('text').text\n",
    "        text_list.append(text_string)\n",
    "        for opinion in review.findall('./Opinions/Opinion'):\n",
    "            opinion_dict = {\n",
    "                opinion.get('category').replace('#','_'): opinion.get('polarity')\n",
    "            }\n",
    "            opinion_inner_list.append(opinion_dict)\n",
    "        opinion_list.append(opinion_inner_list)\n",
    "    return text_list,opinion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_aspect(opinion_list):\n",
    "    import nltk\n",
    "    opinion= []\n",
    "    for inner_list in opinion_list:\n",
    "        for _dict in inner_list:\n",
    "            for key in _dict:\n",
    "                opinion.append(key)\n",
    "    most_common_aspect = [k for k,v in nltk.FreqDist(opinion).most_common(20)]\n",
    "    return most_common_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_frame(text_list,opinion_list,most_common_aspect):\n",
    "    data={'Review':text_list}\n",
    "    df = pd.DataFrame(data)\n",
    "    if opinion_list:\n",
    "        for inner_list in opinion_list:\n",
    "            for _dict in inner_list:\n",
    "                for key in _dict:\n",
    "                    if key in most_common_aspect:\n",
    "                        df.loc[opinion_list.index(inner_list),key]=_dict[key]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive','negative','neutral','conflict'],[1,1,1,1])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive'],[1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative','neutral','conflict'],[0,0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative'],[1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive','neutral','conflict'],[0,0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neutral_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['neutral','conflict'],[1,1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative','positive'],[0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTag(review):\n",
    "    tagged_text_list=[]\n",
    "    for text in review:\n",
    "        tagged_text_list.append(stanford_tag.tag(word_tokenize(text)))\n",
    "    return tagged_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_aspect(y,most_common_aspect):\n",
    "    position=[]\n",
    "    for innerlist in y:\n",
    "        position.append([i for i, j in enumerate(innerlist) if j == 1])\n",
    "    sorted_common=sorted(most_common_aspect)\n",
    "    dict_aspect=[]\n",
    "    for innerlist in position:\n",
    "        inner_dict={}\n",
    "        for word in sorted_common:\n",
    "            if sorted_common.index(word) in innerlist:\n",
    "                inner_dict[word]= 5\n",
    "            else:\n",
    "                inner_dict[word]=0\n",
    "        dict_aspect.append(inner_dict)\n",
    "    return dict_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(l1,l2=None,n=1):\n",
    "    for i in range(0,n):\n",
    "        rand_int = int(np.random.rand() * len(l1))\n",
    "        if l2:\n",
    "            print(l1[rand_int]), '\\n\\n', print(l2[rand_int])\n",
    "        else:\n",
    "            print(l1[rand_int])\n",
    "            return l1[rand_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(l):\n",
    "    print('type: ',type(l))\n",
    "    print('len: ',len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**main control flow in [thestrox Github code](https://github.com/thestrox/Aspect-Based-Sentiment-Analysis/blob/master/Aspect%20Based%20Sentiment%20Analysis.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text_list, train_opinion_list = get_list(path_train)\n",
    "# get_sample(train_text_list,train_opinion_list,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_aspect = get_most_common_aspect(train_opinion_list)\n",
    "# most_common_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text_list_train=joblib.load('tagged_text_list_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_text_list = filterTag(tagged_text_list_train)\n",
    "# get_sample(tagged_text_list_train,final_train_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each train review and its SB2 (aspect,sentiment) pair is POS-tagged, and only nouns, adjectives, verbs and adverbs are retained according to [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) POS table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Review', 'LAPTOP_GENERAL', 'LAPTOP_OPERATION_PERFORMANCE',\n",
       "       'LAPTOP_QUALITY', 'DISPLAY_QUALITY', 'LAPTOP_DESIGN_FEATURES',\n",
       "       'LAPTOP_USABILITY', 'LAPTOP_PRICE', 'COMPANY_GENERAL',\n",
       "       'SUPPORT_QUALITY', 'LAPTOP_CONNECTIVITY', 'LAPTOP_PORTABILITY',\n",
       "       'BATTERY_OPERATION_PERFORMANCE', 'MULTIMEDIA_DEVICES_QUALITY',\n",
       "       'KEYBOARD_DESIGN_FEATURES', 'DISPLAY_GENERAL', 'LAPTOP_MISCELLANEOUS',\n",
       "       'SOFTWARE_GENERAL', 'OS_GENERAL', 'DISPLAY_DESIGN_FEATURES',\n",
       "       'OS_USABILITY'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = get_data_frame(train_text_list,train_opinion_list,most_common_aspect)\n",
    "# df_train.sample(3)\n",
    "# df_train.describe()\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_aspect = get_aspect_data_frame(df_train,most_common_aspect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
