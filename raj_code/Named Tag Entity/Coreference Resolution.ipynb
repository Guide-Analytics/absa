{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
    "!pip install stanza\n",
    "\n",
    "# Import stanza\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Stanford CoreNLP package with Stanza's installation command\n",
    "# This'll take several minutes, depending on the network speed\n",
    "corenlp_dir = './corenlp'\n",
    "# stanza.install_corenlp(dir=corenlp_dir)\n",
    "\n",
    "# Set the CORENLP_HOME environment variable to point to the installation location\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = corenlp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import client module\n",
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-12 21:47:23 INFO: Writing properties to tmp file: corenlp_server-e1d6675da6404693.props\n",
      "2021-01-12 21:47:23 INFO: Starting server with command: java -Xmx4G -cp ./corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e1d6675da6404693.props -annotators coref -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<stanza.server.client.CoreNLPClient object at 0x0000029CD29B21C8>\n"
     ]
    }
   ],
   "source": [
    "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
    "client = CoreNLPClient(\n",
    "    annotators=[\"coref\"], \n",
    "    memory='4G', \n",
    "    endpoint='http://localhost:9001',\n",
    "    be_quiet=True)\n",
    "print(client)\n",
    "\n",
    "# Start the background server and wait for some time\n",
    "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate some text\n",
    "text = \"I really recommend this phone. Great battery life, good camera, 90 fps display, stock android, large display but not bulky at the same time, comes in 5 colours and 2 finishes. It would have been better though if we had an option to upgrade the RAM. It becomes laggy after a few days unless you recharge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It <-> It\n"
     ]
    }
   ],
   "source": [
    "mychains = list()\n",
    "chains = document.corefChain\n",
    "for chain in chains:\n",
    "    mychain = list()\n",
    "    # Loop through every mention of this chain\n",
    "    for mention in chain.mention:\n",
    "        # Get the sentence in which this mention is located, and get the words which are part of this mention\n",
    "        # (we can have more than one word, for example, a mention can be a pronoun like \"he\", but also a compound noun like \"His wife Michelle\")\n",
    "        words_list = document.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
    "        #build a string out of the words of this mention\n",
    "        ment_word = ' '.join([x.word for x in words_list])\n",
    "        mychain.append(ment_word)\n",
    "    mychains.append(mychain)\n",
    "\n",
    "for chain in mychains:\n",
    "    print(' <-> '.join(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: \n",
    "1. https://www.rangakrish.com/index.php/2019/02/10/coreference-resolution-in-stanford-corenlp/\n",
    "2. https://github.com/aleenaraj/Coreference_Resolution/blob/master/CoreNLP_coref.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralCoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy==2.1.0  # version 2.1.0 is only compatible with neuralcoref\n",
    "# !pip install neuralcoref --no-binary neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[this phone: [this phone, Great battery life, good camera, 90 fps display, stock android, large display but not bulky at the same time, 90 fps display, stock android, large display but not bulky, stock android, large display but not bulky, large display but not bulky, the same time, 5 colours and 2 finishes, 2 finishes, It, we, we had an option to upgrade the RAM, an option to upgrade the RAM, the RAM, RAM, It, a few days, you recharge]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "neuralcoref.add_to_pipe(nlp, greedyness=0.6)\n",
    "# Annotate some text\n",
    "# text = \"I bought this light. It works good but battery is not. It discharges quickly.\"\n",
    "doc = nlp(text)\n",
    "print(doc._.coref_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30\n",
    "2. https://github.com/huggingface/neuralcoref\n",
    "3. https://stackoverflow.com/questions/63668616/en-coref-lg-model-in-spacy\n",
    "4. https://www.rangakrish.com/index.php/2019/02/03/coreference-resolution-using-spacy/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
